{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDmOKuq3X7ZZ/G3r7zRbj8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8jvWKHMLRWsS"},"outputs":[],"source":["!pip install visualkeras==0.1.4 keras_tuner==1.4.7 keras_cv==0.9.0 pydot==3.0.4 graphviz==0.20.3 git+https://github.com/raghakot/keras-vis.git"]},{"cell_type":"code","source":["# Kagglehub\n","import kagglehub\n","\n","# Math Modules\n","import numpy as np\n","import pandas as pd\n","\n","# Graph Plotting Modules\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from PIL import ImageOps\n","\n","# Other Python Modules\n","import os\n","import shutil\n","import warnings\n","import ssl\n","warnings.filterwarnings('ignore')\n","ssl._create_default_https_context = ssl._create_unverified_context\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","\n","# Sci Kit Learn\n","from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n","\n","# Tensorflow/Keras\n","import visualkeras\n","import tensorflow as tf\n","import keras_tuner as kt\n","from keras_cv.layers import Augmenter, RandomFlip, RandomRotation\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.callbacks import CSVLogger\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.regularizers import L2\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, AveragePooling2D, MaxPooling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Nadam\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.utils import image_dataset_from_directory\n","from tensorflow.keras.layers import Rescaling\n","\n","# Seaborn Style Settings\n","sns.set_context(\"talk\")\n","sns.set_style(\n","    \"whitegrid\", {'axes.facecolor': '#F0F0D7', 'figure.facecolor': '#727D73'})\n","\n","# Matplotlib Style Settings\n","mpl.rcParams['text.color'] = 'w'\n","mpl.rcParams['xtick.color'] = 'w'\n","mpl.rcParams['ytick.color'] = 'w'\n","mpl.rcParams['axes.labelcolor'] = 'w'\n","\n","# Hyperparameters\n","AUTO = tf.data.AUTOTUNE\n","\n","# Change Data Type for Efficiency\n","policy = tf.keras.mixed_precision.Policy('mixed_float16')\n","tf.keras.mixed_precision.set_global_policy(policy)"],"metadata":{"id":"4EBAk-54uKxW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See if GPU is available\n","print(\"Num. GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"],"metadata":{"id":"g87Rq3qruLo-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Paths if executed in Google Colab\n","# path_data = kagglehub.dataset_download(\"gunavenkatdoddi/eye-diseases-classification\")\n","# path_data += \"/dataset\"\n","# path_tuner = '/content/drive/MyDrive/Self Projects/Eye Disease Classification DL/Hyperparameter Tuning'\n","# path_model = '/content/drive/MyDrive/Self Projects/Eye Disease Classification DL/Models'\n","\n","# Paths if executed in Kaggle\n","path_data = \"/kaggle/input/eye-diseases-classification/dataset\"\n","\n","src_tuner = '/kaggle/input/eye-disease-classification-hyperparameter-tuning/tensorflow2/default/1'\n","src_models = '/kaggle/input/eye-disease-classification-model-and-training-info/tensorflow2/default/1'\n","\n","path_tuner = '/kaggle/working/eye-disease-classification-hyperparameter-tuning/tensorflow2/default/1'\n","path_model = '/kaggle/working/eye-disease-classification-model-and-training-info/tensorflow2/default/1'\n","\n","if not os.path.isdir(path_tuner):\n","    shutil.copytree(src_tuner, path_tuner)\n","if not os.path.isdir(path_model):\n","    shutil.copytree(src_models, path_model)\n","\n","# # Paths if executed in Local PC\n","# path_data = \"./dataset\"\n","# path_tuner = './Hyperparameter Tuning/'\n","# path_model = './Models/'"],"metadata":{"id":"0ghiB-E8uP_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing Dataset & Setting Them In Train, Validation and Test Variables\n","train_ds = image_dataset_from_directory(\n","    directory = path_data,\n","    image_size = (128, 128),\n","    validation_split = 0.3,\n","    subset = \"training\",\n","    seed = 1,\n","    shuffle = True\n",")\n","val_ds = image_dataset_from_directory(\n","    directory = path_data,\n","    image_size = (128, 128),\n","    validation_split = 0.3,\n","    subset = \"validation\",\n","    seed = 1,\n","    shuffle = True\n",")\n","val_batches = tf.data.experimental.cardinality(val_ds)\n","test_ds = val_ds.take((2*val_batches) // 3)\n","val_ds = val_ds.skip((2*val_batches) // 3)\n","\n","# Get Class Names\n","class_names = train_ds.class_names\n","class_names = [i.replace(\"_\", \" \").capitalize() for i in class_names]\n","print('\\nClass Names: ' , class_names)\n","\n","# Data Shape\n","print(\"\\nTraining data shape:\", train_ds.element_spec)\n","print(\"Validation data shape:\", val_ds.element_spec)\n","print(\"Test data shape:\", test_ds.element_spec)"],"metadata":{"id":"gbKX0zf-uRr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Figure Setup\n","fig, axs = plt.subplots(3, 3, figsize=(10, 10), dpi=80)\n","fig.suptitle('Sample Images', fontsize=30)\n","\n","# Plot 9 Different images\n","for images, labels in train_ds.take(1):\n","    for i in range(9):\n","        axs = plt.subplot(3, 3, i + 1)\n","        plt.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray')\n","        plt.title(class_names[labels[i]] + \" (\" + labels[i].numpy().astype(\"str\") + \")\")\n","        plt.axis(\"off\")\n","\n","# Show Images\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"CthjLuAzuZ9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count of each Class in train_ds\n","dataset_unbatched = tuple(train_ds.unbatch())\n","labels = []\n","for (image,label) in dataset_unbatched:\n","    labels.append(class_names[label.numpy()])\n","labels = pd.Series(labels)\n","count = labels.value_counts()\n","\n","count"],"metadata":{"id":"Zt7pQwRwufR-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del dataset_unbatched, labels, count"],"metadata":{"id":"jzJfBrE-uhQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Augmenter\n","augmenter = Augmenter(\n","    layers=[\n","        RandomFlip(seed=123),\n","        RandomRotation(factor=(-1.0, 1.0), fill_mode=\"constant\", seed=123)\n","    ]\n",")\n","\n","# Function To Augment Dataset\n","def augment_data(images, labels):\n","    inputs = {\"images\": images, \"labels\": labels}\n","    outputs = augmenter(inputs)\n","    return outputs['images'], outputs['labels']\n","\n","\n","# Map 'train_ds' To Augment Dataset\n","train_ds_augment = train_ds.map(augment_data, num_parallel_calls=AUTO)\n","\n","# Get Sample Images\n","sample_images, sample_labels = next(iter(train_ds_augment))\n","\n","# Plot 9 Different Sample Images\n","# Plot Size\n","plt.figure(figsize=(10, 10))\n","\n","# For Loop To Plot Images\n","for i, (image, label) in enumerate(zip(sample_images[:9], sample_labels[:9])):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(image.numpy().squeeze().astype(float) / 255)\n","    plt.axis(\"off\")\n","\n","## Combine New Data With Old Data\n","\n","# Cast the images and labels to float32 in both datasets\n","train_ds = train_ds.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32)))\n","train_ds_augment = train_ds_augment.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.float32)))\n","\n","# Now concatenate the datasets\n","train_ds = train_ds.concatenate(train_ds_augment)\n","\n","del train_ds_augment, sample_images, sample_labels, image, label"],"metadata":{"id":"3bk0HXnUukKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Normalize Data\n","normalization_layer = Rescaling(1./255)\n","train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n","val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n","test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n","\n","# Configure Data Performance\n","train_ds = train_ds.cache().prefetch(buffer_size=AUTO)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTO)\n","test_ds = test_ds.cache().prefetch(buffer_size=AUTO)\n","\n","# Configure memory growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","    except:\n","        pass\n","\n","\n","# Hyperparameter Tuning Function\n","def model_builder(hp):\n","\n","    # Hyperparameters\n","    hp_Neuron1 = hp.Int('Neuron1', min_value=32, max_value=512, step=32)\n","    hp_Neuron2 = hp.Int('Neuron2', min_value=32, max_value=512, step=32)\n","    hp_Neuron3 = hp.Int('Neuron3', min_value=32, max_value=512, step=32)\n","    hp_Neuron4 = hp.Int('Neuron4', min_value=32, max_value=512, step=32)\n","    hp_Neuron5 = hp.Int('Neuron5', min_value=32, max_value=512, step=32)\n","    hp_kernel_initializer = hp.Choice('kernel_initializer', values=[\n","                                      'he_uniform', 'glorot_normal'])\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","\n","    # Model Setup\n","    model = Sequential(name='Model_1',layers=[\n","        # Input & First Convolution\n","        Conv2D(filters=hp_Neuron1, kernel_size=5, input_shape=(128, 128, 3),\n","               activation='relu', kernel_initializer=hp_kernel_initializer, kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","        AveragePooling2D(pool_size=(2, 2)),\n","\n","        # 2nd Convolution Layer\n","        Conv2D(hp_Neuron2, kernel_size=4, kernel_initializer=hp_kernel_initializer,\n","               activation='relu', kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","        AveragePooling2D(pool_size=(2, 2)),\n","\n","        # 3rd Convolution Layer\n","        Conv2D(hp_Neuron3, kernel_size=3, kernel_initializer=hp_kernel_initializer,\n","               activation='relu', kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","        AveragePooling2D(pool_size=(2, 2)),\n","        Dropout(0.35, seed=123),\n","\n","        # Flatten\n","        Flatten(),\n","\n","        # 4th Dense Layer\n","        Dense(hp_Neuron4, kernel_initializer=hp_kernel_initializer,\n","              activation='relu', kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","\n","        # 5th Dense Layer\n","        Dense(hp_Neuron5, kernel_initializer=hp_kernel_initializer,\n","              activation='relu', kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","\n","        # Output\n","        Dense(4, activation='softmax', name='output_model_1')\n","    ])\n","\n","    # Model Compile\n","    model.compile(optimizer=Nadam(learning_rate=hp_learning_rate),\n","                  loss=SparseCategoricalCrossentropy(\n","                      from_logits=True),\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","\n","# Tuner\n","tuner = kt.RandomSearch(\n","    model_builder,\n","    objective='val_accuracy',\n","    max_trials=50,\n","    directory=path_tuner,\n","    project_name='Model_1',\n","    seed=12345\n",")\n","\n","# Callbacks\n","stop_early = EarlyStopping(monitor='val_loss', patience=5)\n","\n","# Commence Hyperparameter Tuning\n","tuner.search(train_ds, epochs=3, batch_size=32,\n","             validation_data=val_ds, callbacks=[stop_early])\n","\n","# Get the optimal hyperparameters\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","# Print Results\n","print(f\"\"\"\n","Neuron 1: {best_hps.get('Neuron1')}\n","Neuron 2: {best_hps.get('Neuron2')}\n","Neuron 3: {best_hps.get('Neuron3')}\n","Neuron 4: {best_hps.get('Neuron4')}\n","Neuron 5: {best_hps.get('Neuron5')}\n","Learning Rate: {best_hps.get('learning_rate')}\n","Kernal Initialiser: {best_hps.get('kernel_initializer')}\n","\"\"\")"],"metadata":{"id":"G105F0sFulOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Callbacks\n","monitor_val_acc = EarlyStopping(monitor='val_accuracy', patience=30)\n","csv_logger = CSVLogger(path_model + '/Model_1_Training.csv',\n","                       separator=',', append=False)\n","\n","# If Model & CSV Exists, Skip Model Fitting, Else, Fit Model\n","if os.path.exists(path_model + '/Model_1_Training.h5') and os.path.exists(path_model + '/Model_1_Training.csv'):\n","\n","    # Load Model\n","    model_1 = load_model(path_model + '/Model_1_Training.h5')\n","\n","    # Show Model Summary\n","    model_1.summary()\n","\n","else:\n","\n","    # Get Model From Hyperparameter\n","    model_1 = tuner.hypermodel.build(best_hps)\n","\n","    # Show Model Summary\n","    model_1.summary()\n","\n","    # Fit Model\n","    history = model_1.fit(train_ds, epochs=1000,\n","                        validation_data=val_ds,\n","                        callbacks=[monitor_val_acc, csv_logger])\n","\n","    # Save Model\n","    model_1.save(path_model + '/Model_1_Training.h5')\n","\n","# Load Fitting History\n","history_1 = pd.read_csv(path_model + '/Model_1_Training.csv',\n","                      sep=',', engine='python')"],"metadata":{"id":"6tdoIJ3ruoVe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Visualization\n","visualkeras.layered_view(model_1, legend=True,background_fill='#727D73', font_color='white')"],"metadata":{"id":"UYIBDl94uqim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show Model Plot\n","plot_model(model_1, \"model_1.png\", show_shapes=True, show_layer_names=True,\n","            show_dtype=True, rankdir='LR')"],"metadata":{"id":"FgKVaJxKutVe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accuracy Over Epoch Graph\n","# Figure Setup\n","fig, axs = plt.subplots(figsize=(10, 5), dpi=80)\n","\n","# Plot Accuracy and Val_accuracy Graphs\n","plt.plot(history_1['accuracy'], label='Train')\n","plt.plot(history_1['val_accuracy'], label='Validation')\n","\n","# Labels\n","plt.title('Accuracy of Model_1', fontsize=25)\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1])\n","plt.legend(loc='lower right', labelcolor='black')\n","\n","# Show Graph\n","plt.show()"],"metadata":{"id":"W0mCtqXvuvRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking For Overfitting\n","# Figure Setup\n","plt.figure(figsize=(10, 5), dpi=80)\n","\n","# Plot Accuracy and Val_accuracy loss\n","plt.plot(history_1['loss'])\n","plt.plot(history_1['val_loss'])\n","\n","# Labels\n","plt.title('Loss of Model_1', fontsize=25)\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation', 'Test'],\n","           loc='upper right', labelcolor='black')\n","\n","# Show Graph\n","plt.show()"],"metadata":{"id":"6KjaXS09uxGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Final Testing Dataset Evaluation\n","# Evaluate Model\n","test_loss, test_acc = model_1.evaluate(test_ds, verbose=2)\n","\n","# Get Prediction Percentage\n","prediction = model_1.predict(test_ds)\n","y_classes = prediction.argmax(axis=-1)\n","\n","# Figure Setup\n","sns.set_context('talk')\n","fig, axs = plt.subplots(4, 4, figsize=(\n","    50, 50), dpi=80, constrained_layout=True)\n","fig.suptitle('Random Images with Model\\'s Prediction Percentage', fontsize=100)\n","\n","# Get all data from 'test_ds' instead of in batches\n","x_test = np.concatenate([x for x, y in test_ds], axis=0)\n","y_test = np.concatenate([y for x, y in test_ds], axis=0)\n","\n","# Random Number Generator\n","ran = np.random.randint(0, high=len(x_test), size=(16), dtype=int)\n","\n","# Counter\n","cou = 0\n","\n","# For Loop To Display Random Images With Percentage\n","for i in range(0, 4, 2):\n","    for v in range(0, 4):\n","        # Set Random Images\n","        axs[i, v].imshow(x_test[ran[cou]].astype(float))\n","        n = class_names[y_test[ran[cou]]]\n","        axs[i, v].set_title(n, fontsize=50)\n","        axs[i, v].axis('off')\n","\n","        # Change Colour of Bars According To if Prediction was Right or Wrong\n","        if y_classes[ran[cou]] == y_test[ran[cou]].astype(int):\n","\n","            # Get Predictions For Above Images\n","            axs[i+1, v].bar(class_names, prediction[ran[cou]], color='green')\n","            axs[i+1, v].set_xticklabels(class_names, rotation=90, fontsize=50)\n","            cou = cou + 1\n","        else:\n","\n","            # Get Predictions For Above Images\n","            axs[i+1, v].bar(class_names, prediction[ran[cou]], color='red')\n","            axs[i+1, v].set_xticklabels(class_names, rotation=90, fontsize=50)\n","            cou = cou + 1\n","\n","# Show Plot\n","plt.show()"],"metadata":{"id":"QEIgf4qHuzGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion matrix\n","plt.rcParams[\"axes.grid\"] = False\n","cm = confusion_matrix(y_test, y_classes)\n","ConfusionMatrixDisplay(cm, display_labels=class_names).plot(xticks_rotation='vertical')\n","plt.rcParams[\"axes.grid\"] = True\n","\n","# Classification report\n","report_m1 = classification_report(y_test, y_classes, target_names=class_names)\n","print(report_m1)"],"metadata":{"id":"kc70Als6u11e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameter Tuning Function\n","def model_builder_2(hp):\n","\n","    # Hyperparameters\n","    hp_Neuron1 = hp.Int('Neuron1', min_value=32, max_value=512, step=32)\n","    hp_Neuron2 = hp.Int('Neuron2', min_value=32, max_value=512, step=32)\n","    hp_Neuron3 = hp.Int('Neuron3', min_value=32, max_value=512, step=32)\n","    hp_Neuron4 = hp.Int('Neuron4', min_value=32, max_value=512, step=32)\n","    hp_Neuron5 = hp.Int('Neuron5', min_value=32, max_value=512, step=32)\n","    hp_kernel_initializer = hp.Choice('kernel_initializer', values=[\n","                                      'he_uniform', 'glorot_normal'])\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","\n","    # Model Setup\n","    model = Sequential(name='Model_2', layers=[\n","        # Input & First Convolution\n","        Conv2D(filters=hp_Neuron1, kernel_size=5, input_shape=(128, 128, 3),\n","               activation='relu', kernel_initializer=hp_kernel_initializer, kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","\n","        # 2nd Convolution Layer\n","        Conv2D(hp_Neuron2, kernel_size=4, kernel_initializer=hp_kernel_initializer,\n","               activation='relu', kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","\n","        # 3rd Convolution Layer\n","        Conv2D(hp_Neuron3, kernel_size=3, kernel_initializer=hp_kernel_initializer,\n","               activation='relu', kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","        Dropout(0.35, seed=123),\n","\n","        # Flatten\n","        Flatten(),\n","\n","        # 4th Dense Layer\n","        Dense(hp_Neuron4, kernel_initializer=hp_kernel_initializer,\n","              activation='relu', kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","\n","        # 5th Dense Layer\n","        Dense(hp_Neuron5, kernel_initializer=hp_kernel_initializer,\n","              activation='relu', kernel_regularizer=L2(0.001)),\n","        BatchNormalization(),\n","\n","        # Output\n","        Dense(4, activation='softmax', name='output_model_2')\n","    ])\n","\n","    # Model Compile\n","    model.compile(optimizer=Nadam(learning_rate=hp_learning_rate),\n","                  loss=SparseCategoricalCrossentropy(\n","                      from_logits=True),\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","\n","# Tuner\n","tuner_2 = kt.RandomSearch(\n","    model_builder_2,\n","    objective='val_accuracy',\n","    max_trials=50,\n","    directory=path_tuner,\n","    project_name='Model_2',\n","    seed=12345\n",")\n","\n","# Callbacks\n","stop_early = EarlyStopping(monitor='val_loss', patience=5)\n","\n","# Commence Hyperparameter Tuning\n","tuner_2.search(train_ds, epochs=3, batch_size=32,\n","             validation_data=val_ds, callbacks=[stop_early])\n","\n","# Get the optimal hyperparameters\n","best_hps = tuner_2.get_best_hyperparameters(num_trials=1)[0]\n","\n","# Print Results\n","print(f\"\"\"\n","Neuron 1: {best_hps.get('Neuron1')}\n","Neuron 2: {best_hps.get('Neuron2')}\n","Neuron 3: {best_hps.get('Neuron3')}\n","Neuron 4: {best_hps.get('Neuron4')}\n","Neuron 5: {best_hps.get('Neuron5')}\n","Learning Rate: {best_hps.get('learning_rate')}\n","Kernal Initialiser: {best_hps.get('kernel_initializer')}\n","\"\"\")"],"metadata":{"id":"S42Ku_i1vDf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Callbacks\n","monitor_val_acc = EarlyStopping(monitor='val_accuracy', patience=30)\n","csv_logger = CSVLogger(path_model + '/Model_2_Training.csv',\n","                       separator=',', append=False)\n","\n","# If Model & CSV Exists, Skip Model Fitting, Else, Fit Model\n","if os.path.exists(path_model + '/Model_2_Training.h5') and os.path.exists(path_model + '/Model_2_Training.csv'):\n","\n","    # Load Model\n","    model_2 = load_model(path_model + '/Model_2_Training.h5')\n","\n","    # Show Model Summary\n","    model_2.summary()\n","\n","else:\n","\n","    # Get Model From Hyperparameter\n","    model_2 = tuner_2.hypermodel.build(best_hps)\n","\n","    # Show Model Summary\n","    model_2.summary()\n","\n","    # Fit Model\n","    history = model_2.fit(train_ds, epochs=1000,\n","                        validation_data=val_ds,\n","                        callbacks=[monitor_val_acc, csv_logger])\n","\n","    # Save Model\n","    model_2.save(path_model + '/Model_2_Training.h5')\n","\n","# Load Fitting History\n","history_2 = pd.read_csv(path_model + '/Model_2_Training.csv',\n","                      sep=',', engine='python')"],"metadata":{"id":"_vUpyl4wvHC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Visualization\n","visualkeras.layered_view(model_2, legend=True,background_fill='#727D73', font_color='white')"],"metadata":{"id":"i8R-cw7SvI9-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show Model Plot\n","plot_model(model_2, \"model_2.png\", show_shapes=True, show_layer_names=True,\n","            show_dtype=True, rankdir='LR')"],"metadata":{"id":"Jw41k67nvLJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accuracy Over Epoch Graph\n","# Figure Setup\n","fig, axs = plt.subplots(figsize=(10, 5), dpi=80)\n","\n","# Plot Accuracy and Val_accuracy Graphs\n","plt.plot(history_2['accuracy'], label='Train')\n","plt.plot(history_2['val_accuracy'], label='Validation')\n","\n","# Labels\n","plt.title('Accuracy of Model_2', fontsize=25)\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1])\n","plt.legend(loc='lower right', labelcolor='black')\n","\n","# Show Graph\n","plt.show()"],"metadata":{"id":"aW5uAHyevNN-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking For Overfitting\n","# Figure Setup\n","plt.figure(figsize=(10, 5), dpi=80)\n","\n","# Plot Accuracy and Val_accuracy loss\n","plt.plot(history_2['loss'])\n","plt.plot(history_2['val_loss'])\n","\n","# Labels\n","plt.title('Loss of Model_2', fontsize=25)\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation', 'Test'],\n","           loc='upper right', labelcolor='black')\n","\n","# Show Graph\n","plt.show()"],"metadata":{"id":"lA2fYvPDvPU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Final Testing Dataset Evaluation\n","# Evaluate Model\n","test_loss_2, test_acc_2 = model_2.evaluate(test_ds, verbose=2)\n","\n","# Get Prediction Percentage\n","prediction_2 = model_2.predict(test_ds)\n","y_classes_2 = prediction_2.argmax(axis=-1)\n","\n","# Figure Setup\n","sns.set_context('talk')\n","fig, axs = plt.subplots(4, 4, figsize=(\n","    50, 50), dpi=80, constrained_layout=True)\n","fig.suptitle('Random Images with Model\\'s Prediction Percentage', fontsize=100)\n","\n","# Get all data from 'test_ds' instead of in batches\n","x_test = np.concatenate([x for x, y in test_ds], axis=0)\n","y_test = np.concatenate([y for x, y in test_ds], axis=0)\n","\n","# Random Number Generator\n","ran = np.random.randint(0, high=len(x_test), size=(16), dtype=int)\n","\n","# Counter\n","cou = 0\n","\n","# For Loop To Display Random Images With Percentage\n","for i in range(0, 4, 2):\n","    for v in range(0, 4):\n","        # Set Random Images\n","        axs[i, v].imshow(x_test[ran[cou]].astype(float))\n","        n = class_names[y_test[ran[cou]]]\n","        axs[i, v].set_title(n, fontsize=50)\n","        axs[i, v].axis('off')\n","\n","        # Change Colour of Bars According To if Prediction was Right or Wrong\n","        if y_classes_2[ran[cou]] == y_test[ran[cou]].astype(int):\n","\n","            # Get Predictions For Above Images\n","            axs[i+1, v].bar(class_names, prediction_2[ran[cou]], color='green')\n","            axs[i+1, v].set_xticklabels(class_names, rotation=90, fontsize=50)\n","            cou = cou + 1\n","        else:\n","\n","            # Get Predictions For Above Images\n","            axs[i+1, v].bar(class_names, prediction_2[ran[cou]], color='red')\n","            axs[i+1, v].set_xticklabels(class_names, rotation=90, fontsize=50)\n","            cou = cou + 1\n","\n","# Show Plot\n","plt.show()"],"metadata":{"id":"Lh5qjp4MvRqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion matrix\n","cm_2 = confusion_matrix(y_test, y_classes_2)\n","plt.rcParams[\"axes.grid\"] = False\n","ConfusionMatrixDisplay(cm_2, display_labels=class_names).plot(xticks_rotation='vertical')\n","plt.rcParams[\"axes.grid\"] = True\n","\n","# Classification report\n","report_m2 = classification_report(y_test, y_classes_2, target_names=class_names)\n","print(report_m2)"],"metadata":{"id":"fuK6tImAvULP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print Test Accuracies\n","print(\"Model 1 Test Accuracy:\", test_acc, \"\\nModel 2 Test Accuracy:\" , test_acc_2)\n","\n","# Print Classification Reports\n","print(\"\\nModel 1 Report:\\n\", report_m1 , \"\\n\\nModel 2 Report:\\n\" ,report_m2)\n","\n","# Plot Confusion Matrix, Accuracy & Loss Plots of both Models\n","fig, axs = plt.subplots(2, 2, figsize=(\n","    30, 20), dpi=80, constrained_layout=True)\n","\n","# axs[0,0] Confusion Matrix Model 1\n","ConfusionMatrixDisplay(cm, display_labels=class_names).plot(xticks_rotation='vertical', ax=axs[0, 0])\n","axs[0, 0].set_title(\"Confusion Matrix Model 1\", fontsize=50)\n","axs[0, 0].grid(False)\n","axs[0, 0].set_adjustable('datalim')\n","axs[0, 0].set_facecolor('#727D73')\n","\n","# axs[0,1] Confusion Matrix Model 2\n","ConfusionMatrixDisplay(cm_2, display_labels=class_names).plot(xticks_rotation='vertical', ax=axs[0, 1])\n","axs[0, 1].set_title(\"Confusion Matrix Model 2\", fontsize=50)\n","axs[0, 1].grid(False)\n","axs[0, 1].set_adjustable('datalim')\n","axs[0, 1].set_facecolor('#727D73')\n","\n","# axs[1,0] Accuracy Over Epoch\n","axs[1, 0].plot(history_1['accuracy'], label='Train Model 1')\n","axs[1, 0].plot(history_2['accuracy'], label='Train Model 2')\n","axs[1, 0].plot(history_1['val_accuracy'], label='Validation Model 1')\n","axs[1, 0].plot(history_2['val_accuracy'], label='Validation Model 2')\n","axs[1, 0].set_title('Accuracy', fontsize=50)\n","axs[1, 0].set_xlabel('Epoch')\n","axs[1, 0].set_ylabel('Accuracy')\n","axs[1, 0].set_ylim([0.5, 1])\n","axs[1, 0].legend(loc='lower right', labelcolor='black')\n","\n","# axs[1,1] Loss Over Epoch\n","axs[1, 1].plot(history_1['loss'] , label='loss Model 1')\n","axs[1, 1].plot(history_2['loss'] , label='loss Model 2')\n","axs[1, 1].plot(history_1['val_loss'] , label='Validation loss Model 1')\n","axs[1, 1].plot(history_2['val_loss'] , label='Validation loss Model 2')\n","axs[1, 1].set_title('Loss', fontsize=50)\n","axs[1, 1].set_ylabel('Loss')\n","axs[1, 1].set_xlabel('Epoch')\n","axs[1, 1].legend(loc='upper right', labelcolor='black')\n","\n","# Show Plot\n","plt.show()"],"metadata":{"id":"ug6GNB52vWfu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a Sample Dataset with 100% Accuracy\n","x_sample = np.array([x_test[16],x_test[33],x_test[5],x_test[3]])\n","y_sample = np.array([y_test[16],y_test[33],y_test[5],y_test[3]])\n","model_2.evaluate(x_sample, y_sample)"],"metadata":{"id":"UdY502gMvZYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del x_test, y_test"],"metadata":{"id":"13Rff0VovcFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get Weights of Convolutional Layers\n","for i in model_2.layers:\n","    if 'conv' in i.name:\n","        filters, biases = i.get_weights()\n","        print(i.name, filters.shape)\n","\n","\n","# Plotting Convolutional Layers\n","fig, axs = plt.subplots(3, 3, figsize=(5, 5), dpi=80)\n","fig.suptitle('Convolutional Layers', fontsize=30)\n","\n","# Plot 9 Different images\n","for i in range(9):\n","    axs = plt.subplot(3, 3, i + 1)\n","    axs.imshow(filters[:, :, :, i][:, :, 0].squeeze(), cmap='gray')\n","    axs.axis(\"off\")\n","\n","# Show Images\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"qK_AqlZovd0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a new truncated model to only include the conv layers of interest\n","truncated_model = Sequential()\n","for layer in model_2.layers:\n","    if 'conv' in layer.name:\n","        truncated_model.add(layer)\n","\n","# Generate feature output by predicting on the input image (SELECT NORMAL)\n","feature_output = truncated_model.predict(x_sample[0].reshape(1, 128, 128, 3))\n","\n","columns = 8\n","rows = 8\n","for ftr in feature_output:\n","    fig=plt.figure(figsize=(5, 5))\n","    fig.suptitle('CNN Filter of: '+class_names[y_sample[0]])\n","    for i in range(1, columns*rows +1):\n","        fig =plt.subplot(rows, columns, i)\n","        fig.set_xticks([])  #Turn off axis\n","        fig.set_yticks([])\n","        plt.imshow(ftr[:, :, i-1], cmap='gray')\n","    plt.show()"],"metadata":{"id":"PKUOTImzvf3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate feature output by predicting on the input image (SELECT NORMAL)\n","feature_output = truncated_model.predict(x_sample[1].reshape(1, 128, 128, 3))\n","\n","columns = 8\n","rows = 8\n","for ftr in feature_output:\n","    fig=plt.figure(figsize=(5, 5))\n","    fig.suptitle('CNN Filter of: '+class_names[y_sample[1]])\n","    for i in range(1, columns*rows +1):\n","        fig =plt.subplot(rows, columns, i)\n","        fig.set_xticks([])  #Turn off axis\n","        fig.set_yticks([])\n","        plt.imshow(ftr[:, :, i-1], cmap='gray')\n","    plt.show()"],"metadata":{"id":"LQlAMywuvjuW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate feature output by predicting on the input image (SELECT GLAUCOMA)\n","feature_output = truncated_model.predict(x_sample[2].reshape(1, 128, 128, 3))\n","\n","columns = 8\n","rows = 8\n","for ftr in feature_output:\n","    fig=plt.figure(figsize=(5, 5))\n","    fig.suptitle('CNN Filter of: '+class_names[y_sample[2]])\n","    for i in range(1, columns*rows +1):\n","        fig =plt.subplot(rows, columns, i)\n","        fig.set_xticks([])  #Turn off axis\n","        fig.set_yticks([])\n","        plt.imshow(ftr[:, :, i-1], cmap='gray')\n","    plt.show()"],"metadata":{"id":"af3fJxn7vkVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate feature output by predicting on the input image (SELECT CATARACT)\n","feature_output = truncated_model.predict(x_sample[3].reshape(1, 128, 128, 3))\n","\n","columns = 8\n","rows = 8\n","for ftr in feature_output:\n","    fig=plt.figure(figsize=(5, 5))\n","    fig.suptitle('CNN Filter of: '+class_names[y_sample[3]])\n","    for i in range(1, columns*rows +1):\n","        fig =plt.subplot(rows, columns, i)\n","        fig.set_xticks([])  #Turn off axis\n","        fig.set_yticks([])\n","        plt.imshow(ftr[:, :, i-1], cmap='gray')\n","    plt.show()"],"metadata":{"id":"TAIrh7jZvmOn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer as well as the output predictions\n","    grad_model = tf.keras.models.Model(\n","        inputs = model.inputs,\n","        outputs = [\n","                    model.get_layer(last_conv_layer_name).output,\n","                    model.layers[-1].output,\n","                ]\n","    )\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    # This is the gradient of the output neuron (top predicted or chosen)\n","    # with regard to the output feature map of the last conv layer\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    # then sum all the channels to obtain the heatmap class activation\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","    return heatmap.numpy()\n","\n","def save_and_display_gradcam(img, heatmap, alpha_activation=0.4, alpha_img=1):\n","    # Rescale heatmap to a range 0-255\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # Remove the batch dimension by squeezing the image\n","    img = img.squeeze()\n","\n","    # Use jet colormap to colorize heatmap\n","    jet = mpl.colormaps[\"jet\"]\n","\n","    # Use RGB values of the colormap\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","\n","    # Create an image with RGB colorized heatmap\n","    jet_heatmap = tf.keras.utils.array_to_img(jet_heatmap)\n","    jet_heatmap = jet_heatmap.resize((img.shape[1]-20, img.shape[0]-20))\n","    jet_heatmap = ImageOps.expand(jet_heatmap, border=10, fill='black')\n","    jet_heatmap = tf.keras.utils.img_to_array(jet_heatmap)\n","\n","    # Superimpose the heatmap on original image\n","    superimposed_img = (jet_heatmap * alpha_activation) + ((img * 255) * alpha_img)\n","\n","    return superimposed_img/superimposed_img.max()"],"metadata":{"id":"bBl5-smuvoCW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Duplicate & Remove last layer's activation\n","model_2_viz = tf.keras.models.clone_model(model_2)\n","model_2_viz.layers[-1].activation = None\n","\n","# Plot Sample Image from Each Class With Gradually increasing Activation Overlap\n","fig, axs = plt.subplots(3,4, figsize=(10, 10), dpi=80)\n","fig.suptitle('Sample Image from Each Class with Activation')\n","\n","# Plot Original Image, Activation Image & Overlap Image\n","for i in range(4):\n","    # Reset Alpha\n","    alpha_activation = 0\n","    alpha_img = 1\n","\n","    # Get Image\n","    img = x_sample[i].reshape(1, 128, 128, 3)\n","\n","    # Generate class activation heatmap\n","    heatmap = make_gradcam_heatmap(img, model_2_viz, 'conv2d_5')\n","\n","    for v in range(3):\n","\n","      # Plot Images\n","      axs[v, i].imshow(save_and_display_gradcam(img, heatmap, alpha_activation, alpha_img))\n","      axs[v, i].set_title(class_names[y_sample[i]])\n","      axs[v, i].axis(\"off\")\n","\n","      # Increase Alpha\n","      alpha_activation+=0.5\n","      alpha_img-=0.5\n","\n","# Show Images\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"CFeiJQBdv1q2"},"execution_count":null,"outputs":[]}]}