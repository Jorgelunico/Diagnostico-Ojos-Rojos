{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6482249,
     "status": "ok",
     "timestamp": 1752355707304,
     "user": {
      "displayName": "JORGE LUIS YOVERA CHAVEZ",
      "userId": "14550606723561152617"
     },
     "user_tz": 300
    },
    "id": "og9Ejf9qaayy",
    "outputId": "9728a300-c225-4f16-de4f-e64550092aa8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "from statsmodels.stats.contingency_tables import mcnemar \n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "VALIDATION_DATA_DIR = '/content/drive/MyDrive/dataset'\n",
    "\n",
    "MODEL1_PATH = '/content/drive/MyDrive/models/Model_1_Training.h5'\n",
    "MODEL2_PATH = '/content/drive/MyDrive/models/Model_2_Training.h5'\n",
    "MODEL3_PATH = '/content/drive/MyDrive/models/Model_3_Training.keras'\n",
    "\n",
    "CLASSES = ['cataract', 'diabetic_retinopathy', 'glaucoma', 'normal'] # Ejemplo: ajusta a tus clases reales\n",
    "\n",
    "models_to_evaluate = [\n",
    "    {'name': 'Modelo 1', 'path': MODEL1_PATH, 'img_size': (128, 128)},\n",
    "    {'name': 'Modelo 2', 'path': MODEL2_PATH, 'img_size': (128, 128)},\n",
    "    {'name': 'Modelo 3', 'path': MODEL3_PATH, 'img_size': (256, 256)}  \n",
    "\n",
    "loaded_models = {}\n",
    "\n",
    "print(\"Cargando modelos con sus tamaños de imagen específicos...\")\n",
    "for model_info in models_to_evaluate:\n",
    "    model_name = model_info['name']\n",
    "    model_path = model_info['path']\n",
    "    model_img_size = model_info['img_size']\n",
    "    print(f\"Intentando cargar {model_name} desde {model_path}...\")\n",
    "    try:\n",
    "        loaded_model = load_model(model_path)\n",
    "        loaded_models[model_name] = loaded_model\n",
    "        print(f\"{model_name} cargado exitosamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {model_name} desde {model_path}: {e}\")\n",
    "        loaded_models[model_name] = None \n",
    "\n",
    "print(\"Carga de modelos completada.\")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "import os\n",
    "if not os.path.exists(VALIDATION_DATA_DIR):\n",
    "    print(f\"Error: Directory not found at {VALIDATION_DATA_DIR}\")\n",
    "\n",
    "if models_to_evaluate and os.path.exists(VALIDATION_DATA_DIR):\n",
    "    try:\n",
    "        temp_generator = validation_datagen.flow_from_directory(\n",
    "            VALIDATION_DATA_DIR,\n",
    "            target_size=models_to_evaluate[0]['img_size'], \n",
    "            batch_size=1, \n",
    "            class_mode='categorical',\n",
    "            shuffle=False\n",
    "        )\n",
    "        y_true = temp_generator.classes\n",
    "        print(f\"Clases inferidas por el generador para etiquetas: {temp_generator.class_indices}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating temporary generator to get true labels: {e}\")\n",
    "        y_true = None \n",
    "else:\n",
    "    y_true = None \n",
    "\n",
    "def evaluate_model_performance(model, y_true_labels, model_name, class_names, validation_data_dir, target_img_size, batch_size=32):\n",
    "    if model is None:\n",
    "        print(f\"\\n--- Skipping evaluation for {model_name} as it failed to load ---\")\n",
    "        return None, None \n",
    "    if y_true_labels is None:\n",
    "        print(f\"\\n--- Skipping evaluation for {model_name} as true labels could not be obtained ---\")\n",
    "        return None, None \n",
    "    if not os.path.exists(validation_data_dir):\n",
    "         print(f\"\\n--- Skipping evaluation for {model_name} as validation data directory not found ---\")\n",
    "         return None, None\n",
    "\n",
    "    print(f\"\\n--- Evaluando {model_name} ---\")\n",
    "\n",
    "    model_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    model_generator = model_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=target_img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False \n",
    "    )\n",
    "\n",
    "    steps = int(np.ceil(model_generator.n / model_generator.batch_size)) \n",
    "    predictions_raw = model.predict(model_generator, steps=steps, verbose=1)\n",
    "    y_pred_labels = np.argmax(predictions_raw, axis=1)\n",
    "    cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "    fig, ax = plt.subplots(figsize=(len(class_names)+2, len(class_names)+2))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "    ax.set_title(f'Matriz de Confusión - {model_name}')\n",
    "    ax.set_ylabel('Etiqueta Verdadera')\n",
    "    ax.set_xlabel('Etiqueta Predicha')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    confusion_matrix_filename = f\"{model_name.replace(' ', '_')}_confusion_matrix.png\"\n",
    "    plt.savefig(confusion_matrix_filename)\n",
    "    print(f\"Matriz de confusión guardada como {confusion_matrix_filename}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    report = classification_report(y_true_labels, y_pred_labels,\n",
    "                                   target_names=class_names, output_dict=True, zero_division=0)\n",
    "\n",
    "    precision = report['weighted avg']['precision']\n",
    "    sensibilidad = report['weighted avg']['recall']\n",
    "    f1_score_val = report['weighted avg']['f1-score']\n",
    "    mcc = matthews_corrcoef(y_true_labels, y_pred_labels)\n",
    "    total_specificity = 0\n",
    "    num_classes = len(class_names)\n",
    "    for i in range(num_classes):\n",
    "        TN = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
    "        FP = np.sum(cm[:, i]) - cm[i, i]\n",
    "\n",
    "        if (TN + FP) > 0:\n",
    "            total_specificity += TN / (TN + FP)\n",
    "    specificity = total_specificity / num_classes if num_classes > 0 else 0\n",
    "\n",
    "    metrics = {\n",
    "        'Modelo': model_name,\n",
    "        'Precision': f\"{precision:.3f}\",\n",
    "        'Sensibilidad': f\"{sensibilidad:.3f}\",\n",
    "        'Especificidad': f\"{specificity:.3f}\",\n",
    "        'F1-Score': f\"{f1_score_val:.3f}\",\n",
    "        'MCC': f\"{mcc:.3f}\"\n",
    "    }\n",
    "\n",
    "    return metrics, y_pred_labels\n",
    "\n",
    "results = []\n",
    "model_predictions = {} \n",
    "\n",
    "if y_true is not None:\n",
    "    for model_info in models_to_evaluate:\n",
    "        model_name = model_info['name']\n",
    "        model = loaded_models.get(model_name) \n",
    "        model_img_size = model_info['img_size']\n",
    "        result, predictions = evaluate_model_performance(\n",
    "            model=model,\n",
    "            y_true_labels=y_true,\n",
    "            model_name=model_name,\n",
    "            class_names=CLASSES,\n",
    "            validation_data_dir=VALIDATION_DATA_DIR,\n",
    "            target_img_size=model_img_size,\n",
    "            batch_size=32 \n",
    "        )\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            if predictions is not None:\n",
    "                model_predictions[model_name] = predictions\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipping model evaluation because true labels could not be obtained.\")\n",
    "\n",
    "if results: \n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\n--- Tabla Comparativa de Métricas ---\")\n",
    "    print(df_results.to_string(index=False)) \n",
    "else:\n",
    "    print(\"\\nNo results to display.\")\n",
    "\n",
    "print(\"\\n--- Resultados de la Prueba de McNemar (Comparación Pareada de Modelos) ---\")\n",
    "\n",
    "if len(model_predictions) >= 2:\n",
    "    model_names = list(model_predictions.keys())\n",
    "    mcnemar_results = []\n",
    "\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i + 1, len(model_names)):\n",
    "            model1_name = model_names[i]\n",
    "            model2_name = model_names[j]\n",
    "            predictions1 = model_predictions[model1_name]\n",
    "            predictions2 = model_predictions[model2_name]\n",
    "            a = np.sum((predictions1 == y_true) & (predictions2 == y_true))\n",
    "            b = np.sum((predictions1 == y_true) & (predictions2 != y_true))\n",
    "            c = np.sum((predictions1 != y_true) & (predictions2 == y_true))\n",
    "            d = np.sum((predictions1 != y_true) & (predictions2 != y_true))\n",
    "            contingency_table = [[a, b], [c, d]]\n",
    "\n",
    "            try:\n",
    "                result = mcnemar(contingency_table, exact=True) \n",
    "                mcnemar_results.append({\n",
    "                    'Modelos Comparados': f\"{model1_name} vs {model2_name}\",\n",
    "                    'Chi-square statistic': f\"{result.statistic:.3f}\",\n",
    "                    'P-value': f\"{result.pvalue:.4f}\",\n",
    "                    'Significancia (alpha=0.05)': 'Sí' if result.pvalue < 0.05 else 'No'\n",
    "                })\n",
    "            except ValueError as e:\n",
    "                 mcnemar_results.append({\n",
    "                    'Modelos Comparados': f\"{model1_name} vs {model2_name}\",\n",
    "                    'Chi-square statistic': 'N/A',\n",
    "                    'P-value': 'N/A',\n",
    "                    'Significancia (alpha=0.05)': f\"Error: {e}\"\n",
    "                })\n",
    "\n",
    "    if mcnemar_results:\n",
    "        df_mcnemar = pd.DataFrame(mcnemar_results)\n",
    "        print(df_mcnemar.to_string(index=False))\n",
    "    else:\n",
    "        print(\"Could not perform McNemar's test. Ensure models were loaded and evaluated successfully.\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough models with valid predictions to perform McNemar's test.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMGc06/8dCwWCM+mDRTpA7S",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
